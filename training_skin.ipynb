{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-30 23:33:12.538251: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-30 23:33:12.538308: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-30 23:33:12.539953: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-30 23:33:12.548812: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-30 23:33:14.188501: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21437 images belonging to 10 classes.\n",
      "Found 5272 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "train_dir = \"train\"\n",
    "validation_dir = \"val\"\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  \n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=False\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),  \n",
    "    batch_size=32,\n",
    "    class_mode='categorical'  \n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-25 01:19:49.881545: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-25 01:19:49.881777: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-25 01:19:49.910895: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-25 01:19:49.981686: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-25 01:19:51.679340: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-03-25 01:19:54.519387: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-25 01:19:54.830203: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-25 01:19:54.830744: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-25 01:30:30.862464: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-25 01:30:30.866067: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-25 01:30:30.866398: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-25 01:30:31.176475: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-25 01:30:31.177483: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-25 01:30:31.178212: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-25 01:30:31.178390: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2796 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "219055592/219055592 [==============================] - 53s 0us/step\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Conv2D' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/media/virendrasinh10/New Volume/ISIC_2019_Training_Input/new_data/tr.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/virendrasinh10/New%20Volume/ISIC_2019_Training_Input/new_data/tr.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m model2\u001b[39m.\u001b[39madd(base_model)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/virendrasinh10/New%20Volume/ISIC_2019_Training_Input/new_data/tr.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Freeze the convolutional base\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/media/virendrasinh10/New%20Volume/ISIC_2019_Training_Input/new_data/tr.ipynb#W5sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m base_model\u001b[39m.\u001b[39mlayers[\u001b[39m-\u001b[39m\u001b[39m5\u001b[39m]:\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/virendrasinh10/New%20Volume/ISIC_2019_Training_Input/new_data/tr.ipynb#W5sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     layer\u001b[39m.\u001b[39mtrainable \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/virendrasinh10/New%20Volume/ISIC_2019_Training_Input/new_data/tr.ipynb#W5sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m model2\u001b[39m.\u001b[39madd(GlobalAveragePooling2D())\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Conv2D' object is not iterable"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import InceptionResNetV2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "\n",
    "\n",
    "input_shape = (224, 224, 3)  \n",
    "\n",
    "model2 = Sequential()\n",
    "\n",
    "base_model = InceptionResNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "for layer in base_model.layers[-5]:\n",
    "    layer.trainable = False\n",
    "model2.add(base_model)\n",
    "\n",
    "\n",
    "model2.add(GlobalAveragePooling2D())\n",
    "model2.add(Dense(1024, activation='relu'))  \n",
    "model2.add(Dense(512, activation='relu'))  \n",
    "model2.add(Dense(128, activation='relu'))  \n",
    "model2.add(Dense(10, activation='softmax'))  \n",
    "\n",
    "model2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " densenet121 (Functional)    (None, None, None, 1024   7037504   \n",
      "                             )                                   \n",
      "                                                                 \n",
      " global_average_pooling2d_1  (None, 1024)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               131200    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7169994 (27.35 MB)\n",
      "Trainable params: 171402 (669.54 KB)\n",
      "Non-trainable params: 6998592 (26.70 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "\n",
    "base_model = DenseNet121(weights='imagenet', include_top=False)\n",
    "for layer in base_model.layers[:-5]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(base_model)\n",
    "\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))  # Assuming 10 classes\n",
    "\n",
    "\n",
    "model.compile(optimizer=Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', Precision(), Recall()],)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "670/670 [==============================] - 1417s 2s/step - loss: 1.4615 - accuracy: 0.4785 - precision: 0.7361 - recall: 0.2663 - val_loss: 1.3578 - val_accuracy: 0.5135 - val_precision: 0.6944 - val_recall: 0.3557\n",
      "Epoch 2/20\n",
      "670/670 [==============================] - 1285s 2s/step - loss: 1.2745 - accuracy: 0.5370 - precision: 0.7454 - recall: 0.3493 - val_loss: 1.2041 - val_accuracy: 0.5679 - val_precision: 0.7377 - val_recall: 0.3911\n",
      "Epoch 3/20\n",
      "670/670 [==============================] - 1275s 2s/step - loss: 1.1817 - accuracy: 0.5687 - precision: 0.7557 - recall: 0.3911 - val_loss: 1.2196 - val_accuracy: 0.5597 - val_precision: 0.7199 - val_recall: 0.4270\n",
      "Epoch 4/20\n",
      "670/670 [==============================] - 1280s 2s/step - loss: 1.1192 - accuracy: 0.5976 - precision: 0.7660 - recall: 0.4240 - val_loss: 1.1429 - val_accuracy: 0.5880 - val_precision: 0.7483 - val_recall: 0.4427\n",
      "Epoch 5/20\n",
      "670/670 [==============================] - 1284s 2s/step - loss: 1.0686 - accuracy: 0.6121 - precision: 0.7715 - recall: 0.4529 - val_loss: 1.0760 - val_accuracy: 0.6142 - val_precision: 0.7585 - val_recall: 0.4742\n",
      "Epoch 6/20\n",
      "670/670 [==============================] - 1284s 2s/step - loss: 1.0338 - accuracy: 0.6223 - precision: 0.7737 - recall: 0.4725 - val_loss: 1.0466 - val_accuracy: 0.6259 - val_precision: 0.7663 - val_recall: 0.5000\n",
      "Epoch 7/20\n",
      "670/670 [==============================] - 1284s 2s/step - loss: 0.9942 - accuracy: 0.6385 - precision: 0.7819 - recall: 0.4976 - val_loss: 1.0278 - val_accuracy: 0.6307 - val_precision: 0.7886 - val_recall: 0.4953\n",
      "Epoch 8/20\n",
      "670/670 [==============================] - 1280s 2s/step - loss: 0.9586 - accuracy: 0.6530 - precision: 0.7915 - recall: 0.5185 - val_loss: 1.0358 - val_accuracy: 0.6288 - val_precision: 0.7463 - val_recall: 0.5201\n",
      "Epoch 9/20\n",
      "670/670 [==============================] - 1280s 2s/step - loss: 0.9338 - accuracy: 0.6624 - precision: 0.7916 - recall: 0.5352 - val_loss: 1.0008 - val_accuracy: 0.6375 - val_precision: 0.7573 - val_recall: 0.5267\n",
      "Epoch 10/20\n",
      "670/670 [==============================] - 1276s 2s/step - loss: 0.9083 - accuracy: 0.6687 - precision: 0.7969 - recall: 0.5452 - val_loss: 1.0226 - val_accuracy: 0.6434 - val_precision: 0.7376 - val_recall: 0.5577\n",
      "Epoch 11/20\n",
      "670/670 [==============================] - 1284s 2s/step - loss: 0.8796 - accuracy: 0.6766 - precision: 0.7951 - recall: 0.5605 - val_loss: 1.0177 - val_accuracy: 0.6434 - val_precision: 0.7456 - val_recall: 0.5438\n",
      "Epoch 12/20\n",
      "670/670 [==============================] - 1277s 2s/step - loss: 0.8553 - accuracy: 0.6868 - precision: 0.8025 - recall: 0.5727 - val_loss: 1.0279 - val_accuracy: 0.6337 - val_precision: 0.7380 - val_recall: 0.5347\n",
      "Epoch 13/20\n",
      "670/670 [==============================] - 1278s 2s/step - loss: 0.8381 - accuracy: 0.6918 - precision: 0.8017 - recall: 0.5818 - val_loss: 0.9470 - val_accuracy: 0.6591 - val_precision: 0.7696 - val_recall: 0.5607\n",
      "Epoch 14/20\n",
      "670/670 [==============================] - 1278s 2s/step - loss: 0.8122 - accuracy: 0.7024 - precision: 0.8066 - recall: 0.5981 - val_loss: 0.9626 - val_accuracy: 0.6552 - val_precision: 0.7685 - val_recall: 0.5565\n",
      "Epoch 15/20\n",
      "670/670 [==============================] - 1270s 2s/step - loss: 0.7975 - accuracy: 0.7083 - precision: 0.8122 - recall: 0.6062 - val_loss: 0.9629 - val_accuracy: 0.6595 - val_precision: 0.7441 - val_recall: 0.5730\n",
      "Epoch 16/20\n",
      "670/670 [==============================] - 1270s 2s/step - loss: 0.7890 - accuracy: 0.7107 - precision: 0.8084 - recall: 0.6143 - val_loss: 0.9088 - val_accuracy: 0.6739 - val_precision: 0.7641 - val_recall: 0.5996\n",
      "Epoch 17/20\n",
      "670/670 [==============================] - 1279s 2s/step - loss: 0.7678 - accuracy: 0.7192 - precision: 0.8112 - recall: 0.6241 - val_loss: 0.9192 - val_accuracy: 0.6730 - val_precision: 0.7554 - val_recall: 0.5928\n",
      "Epoch 18/20\n",
      "670/670 [==============================] - 1266s 2s/step - loss: 0.7476 - accuracy: 0.7265 - precision: 0.8208 - recall: 0.6346 - val_loss: 0.9525 - val_accuracy: 0.6728 - val_precision: 0.7553 - val_recall: 0.6041\n",
      "Epoch 19/20\n",
      "670/670 [==============================] - 1274s 2s/step - loss: 0.7369 - accuracy: 0.7287 - precision: 0.8193 - recall: 0.6401 - val_loss: 0.9154 - val_accuracy: 0.6804 - val_precision: 0.7604 - val_recall: 0.6051\n",
      "Epoch 20/20\n",
      "670/670 [==============================] - 1285s 2s/step - loss: 0.7251 - accuracy: 0.7326 - precision: 0.8203 - recall: 0.6471 - val_loss: 0.9762 - val_accuracy: 0.6650 - val_precision: 0.7469 - val_recall: 0.5910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/virendrasinh10/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_generator,\n",
    "          epochs=20,\n",
    "          validation_data=validation_generator)\n",
    "try:\n",
    "    model.save(\"mymod.h5\")\n",
    "except:\n",
    "    model.save(\"/home/virendrasinh10/newmod.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " densenet169 (Functional)    (None, 7, 7, 1664)        12642880  \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 1664)              0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              1704960   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               65664     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14939594 (56.99 MB)\n",
      "Trainable params: 2546058 (9.71 MB)\n",
      "Non-trainable params: 12393536 (47.28 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import DenseNet169\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "\n",
    "input_shape = (224, 224, 3)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "base_model = DenseNet169(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "for layer in base_model.layers[:-7]:\n",
    "    layer.trainable = False\n",
    "model.add(base_model)\n",
    "\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "\n",
    "model.compile(optimizer=Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', Precision(), Recall()],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-30 01:01:21.098512: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 19267584 exceeds 10% of free system memory.\n",
      "2024-03-30 01:01:33.490291: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 19267584 exceeds 10% of free system memory.\n",
      "2024-03-30 01:01:33.670768: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2024-03-30 01:01:33.778175: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-03-30 01:01:33.901718: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-03-30 01:01:35.026910: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 19267584 exceeds 10% of free system memory.\n",
      "2024-03-30 01:01:36.124490: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 19267584 exceeds 10% of free system memory.\n",
      "2024-03-30 01:01:44.628964: I external/local_xla/xla/service/service.cc:168] XLA service 0x784909a9b450 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-03-30 01:01:44.629062: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1650, Compute Capability 7.5\n",
      "2024-03-30 01:01:44.659547: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1711740704.847498  378786 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4/670 [..............................] - ETA: 8:01 - loss: 2.4768 - accuracy: 0.2109 - precision: 0.8333 - recall: 0.0781         "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-30 01:01:48.490987: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 19267584 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "670/670 [==============================] - 1399s 2s/step - loss: 1.4027 - accuracy: 0.4937 - precision: 0.7384 - recall: 0.2999 - val_loss: 1.1680 - val_accuracy: 0.5833 - val_precision: 0.7664 - val_recall: 0.3964\n",
      "Epoch 2/40\n",
      "670/670 [==============================] - 1362s 2s/step - loss: 1.1096 - accuracy: 0.5953 - precision: 0.7530 - recall: 0.4425 - val_loss: 1.1678 - val_accuracy: 0.5867 - val_precision: 0.7054 - val_recall: 0.4723\n",
      "Epoch 3/40\n",
      "670/670 [==============================] - 1363s 2s/step - loss: 0.9702 - accuracy: 0.6484 - precision: 0.7774 - recall: 0.5184 - val_loss: 0.9317 - val_accuracy: 0.6737 - val_precision: 0.7792 - val_recall: 0.5802\n",
      "Epoch 4/40\n",
      "670/670 [==============================] - 1356s 2s/step - loss: 0.8724 - accuracy: 0.6796 - precision: 0.7878 - recall: 0.5683 - val_loss: 0.9795 - val_accuracy: 0.6538 - val_precision: 0.7484 - val_recall: 0.5641\n",
      "Epoch 5/40\n",
      "670/670 [==============================] - 1548s 2s/step - loss: 0.7883 - accuracy: 0.7112 - precision: 0.8037 - recall: 0.6141 - val_loss: 0.8870 - val_accuracy: 0.6882 - val_precision: 0.7672 - val_recall: 0.6187\n",
      "Epoch 6/40\n",
      "670/670 [==============================] - 1488s 2s/step - loss: 0.7348 - accuracy: 0.7301 - precision: 0.8147 - recall: 0.6458 - val_loss: 1.0689 - val_accuracy: 0.6591 - val_precision: 0.7278 - val_recall: 0.6019\n",
      "Epoch 7/40\n",
      "670/670 [==============================] - 1331s 2s/step - loss: 0.6779 - accuracy: 0.7531 - precision: 0.8252 - recall: 0.6783 - val_loss: 0.9682 - val_accuracy: 0.6766 - val_precision: 0.7507 - val_recall: 0.6151\n",
      "Epoch 8/40\n",
      "670/670 [==============================] - 1332s 2s/step - loss: 0.6265 - accuracy: 0.7672 - precision: 0.8307 - recall: 0.7031 - val_loss: 0.8102 - val_accuracy: 0.7240 - val_precision: 0.7781 - val_recall: 0.6753\n",
      "Epoch 9/40\n",
      "670/670 [==============================] - 1323s 2s/step - loss: 0.5893 - accuracy: 0.7839 - precision: 0.8436 - recall: 0.7259 - val_loss: 0.7840 - val_accuracy: 0.7259 - val_precision: 0.7991 - val_recall: 0.6720\n",
      "Epoch 10/40\n",
      "670/670 [==============================] - 1304s 2s/step - loss: 0.5423 - accuracy: 0.8008 - precision: 0.8539 - recall: 0.7478 - val_loss: 0.8028 - val_accuracy: 0.7420 - val_precision: 0.7830 - val_recall: 0.7003\n",
      "Epoch 11/40\n",
      "670/670 [==============================] - 1321s 2s/step - loss: 0.5095 - accuracy: 0.8135 - precision: 0.8581 - recall: 0.7670 - val_loss: 0.7955 - val_accuracy: 0.7472 - val_precision: 0.7918 - val_recall: 0.7104\n",
      "Epoch 12/40\n",
      "670/670 [==============================] - 1318s 2s/step - loss: 0.4771 - accuracy: 0.8249 - precision: 0.8701 - recall: 0.7832 - val_loss: 0.7929 - val_accuracy: 0.7434 - val_precision: 0.7884 - val_recall: 0.6995\n",
      "Epoch 13/40\n",
      "670/670 [==============================] - 1330s 2s/step - loss: 0.4471 - accuracy: 0.8354 - precision: 0.8747 - recall: 0.7972 - val_loss: 0.7578 - val_accuracy: 0.7623 - val_precision: 0.7993 - val_recall: 0.7238\n",
      "Epoch 14/40\n",
      "670/670 [==============================] - 1318s 2s/step - loss: 0.4157 - accuracy: 0.8477 - precision: 0.8827 - recall: 0.8163 - val_loss: 0.8205 - val_accuracy: 0.7426 - val_precision: 0.7790 - val_recall: 0.7142\n",
      "Epoch 15/40\n",
      "670/670 [==============================] - 1328s 2s/step - loss: 0.3814 - accuracy: 0.8609 - precision: 0.8913 - recall: 0.8329 - val_loss: 0.8648 - val_accuracy: 0.7475 - val_precision: 0.7753 - val_recall: 0.7225\n",
      "Epoch 16/40\n",
      "670/670 [==============================] - 1329s 2s/step - loss: 0.3643 - accuracy: 0.8681 - precision: 0.8960 - recall: 0.8418 - val_loss: 0.8684 - val_accuracy: 0.7513 - val_precision: 0.7838 - val_recall: 0.7284\n",
      "Epoch 17/40\n",
      "670/670 [==============================] - 1308s 2s/step - loss: 0.3470 - accuracy: 0.8737 - precision: 0.9012 - recall: 0.8498 - val_loss: 0.9080 - val_accuracy: 0.7449 - val_precision: 0.7720 - val_recall: 0.7259\n",
      "Epoch 18/40\n",
      "670/670 [==============================] - 1317s 2s/step - loss: 0.3342 - accuracy: 0.8776 - precision: 0.9026 - recall: 0.8569 - val_loss: 0.7991 - val_accuracy: 0.7750 - val_precision: 0.8029 - val_recall: 0.7549\n",
      "Epoch 19/40\n",
      "670/670 [==============================] - 1320s 2s/step - loss: 0.3075 - accuracy: 0.8868 - precision: 0.9080 - recall: 0.8665 - val_loss: 0.8177 - val_accuracy: 0.7673 - val_precision: 0.7972 - val_recall: 0.7479\n",
      "Epoch 20/40\n",
      "670/670 [==============================] - 1338s 2s/step - loss: 0.3007 - accuracy: 0.8918 - precision: 0.9121 - recall: 0.8726 - val_loss: 0.8256 - val_accuracy: 0.7766 - val_precision: 0.8009 - val_recall: 0.7570\n",
      "Epoch 21/40\n",
      "670/670 [==============================] - 1330s 2s/step - loss: 0.2838 - accuracy: 0.8970 - precision: 0.9152 - recall: 0.8814 - val_loss: 0.8665 - val_accuracy: 0.7671 - val_precision: 0.7914 - val_recall: 0.7498\n",
      "Epoch 22/40\n",
      "670/670 [==============================] - 1322s 2s/step - loss: 0.2690 - accuracy: 0.9028 - precision: 0.9208 - recall: 0.8879 - val_loss: 0.9108 - val_accuracy: 0.7631 - val_precision: 0.7832 - val_recall: 0.7477\n",
      "Epoch 23/40\n",
      "670/670 [==============================] - 1321s 2s/step - loss: 0.2566 - accuracy: 0.9060 - precision: 0.9218 - recall: 0.8914 - val_loss: 0.8268 - val_accuracy: 0.7760 - val_precision: 0.7984 - val_recall: 0.7595\n",
      "Epoch 24/40\n",
      "670/670 [==============================] - 1309s 2s/step - loss: 0.2524 - accuracy: 0.9125 - precision: 0.9269 - recall: 0.8978 - val_loss: 0.8678 - val_accuracy: 0.7781 - val_precision: 0.7968 - val_recall: 0.7675\n",
      "Epoch 25/40\n",
      "670/670 [==============================] - 1327s 2s/step - loss: 0.2472 - accuracy: 0.9118 - precision: 0.9261 - recall: 0.8992 - val_loss: 0.9087 - val_accuracy: 0.7733 - val_precision: 0.7942 - val_recall: 0.7589\n",
      "Epoch 26/40\n",
      "670/670 [==============================] - 1330s 2s/step - loss: 0.2257 - accuracy: 0.9201 - precision: 0.9332 - recall: 0.9081 - val_loss: 0.8532 - val_accuracy: 0.7771 - val_precision: 0.7960 - val_recall: 0.7616\n",
      "Epoch 27/40\n",
      "670/670 [==============================] - 1326s 2s/step - loss: 0.2188 - accuracy: 0.9229 - precision: 0.9340 - recall: 0.9120 - val_loss: 0.8278 - val_accuracy: 0.7896 - val_precision: 0.8077 - val_recall: 0.7783\n",
      "Epoch 28/40\n",
      "670/670 [==============================] - 1360s 2s/step - loss: 0.2084 - accuracy: 0.9253 - precision: 0.9372 - recall: 0.9148 - val_loss: 0.8439 - val_accuracy: 0.7830 - val_precision: 0.8032 - val_recall: 0.7718\n",
      "Epoch 29/40\n",
      "670/670 [==============================] - 1462s 2s/step - loss: 0.2043 - accuracy: 0.9272 - precision: 0.9371 - recall: 0.9180 - val_loss: 0.9692 - val_accuracy: 0.7712 - val_precision: 0.7876 - val_recall: 0.7597\n",
      "Epoch 30/40\n",
      "670/670 [==============================] - 1363s 2s/step - loss: 0.2018 - accuracy: 0.9272 - precision: 0.9370 - recall: 0.9177 - val_loss: 0.8909 - val_accuracy: 0.7783 - val_precision: 0.7962 - val_recall: 0.7671\n",
      "Epoch 31/40\n",
      "670/670 [==============================] - 1431s 2s/step - loss: 0.1816 - accuracy: 0.9349 - precision: 0.9440 - recall: 0.9279 - val_loss: 0.9996 - val_accuracy: 0.7743 - val_precision: 0.7929 - val_recall: 0.7667\n",
      "Epoch 32/40\n",
      "670/670 [==============================] - 1340s 2s/step - loss: 0.1844 - accuracy: 0.9338 - precision: 0.9430 - recall: 0.9264 - val_loss: 0.9152 - val_accuracy: 0.7879 - val_precision: 0.8023 - val_recall: 0.7773\n",
      "Epoch 33/40\n",
      "670/670 [==============================] - 1253s 2s/step - loss: 0.1884 - accuracy: 0.9346 - precision: 0.9434 - recall: 0.9260 - val_loss: 0.8814 - val_accuracy: 0.7883 - val_precision: 0.8066 - val_recall: 0.7783\n",
      "Epoch 34/40\n",
      "670/670 [==============================] - 1247s 2s/step - loss: 0.1727 - accuracy: 0.9389 - precision: 0.9469 - recall: 0.9322 - val_loss: 1.0419 - val_accuracy: 0.7821 - val_precision: 0.7937 - val_recall: 0.7701\n",
      "Epoch 35/40\n",
      "670/670 [==============================] - 1345s 2s/step - loss: 0.1703 - accuracy: 0.9415 - precision: 0.9488 - recall: 0.9344 - val_loss: 0.9695 - val_accuracy: 0.7834 - val_precision: 0.7982 - val_recall: 0.7743\n",
      "Epoch 36/40\n",
      "670/670 [==============================] - 1358s 2s/step - loss: 0.1604 - accuracy: 0.9422 - precision: 0.9495 - recall: 0.9353 - val_loss: 1.1513 - val_accuracy: 0.7748 - val_precision: 0.7912 - val_recall: 0.7669\n",
      "Epoch 37/40\n",
      "670/670 [==============================] - 1359s 2s/step - loss: 0.1576 - accuracy: 0.9474 - precision: 0.9537 - recall: 0.9416 - val_loss: 0.9851 - val_accuracy: 0.7769 - val_precision: 0.7883 - val_recall: 0.7676\n",
      "Epoch 38/40\n",
      "670/670 [==============================] - 1365s 2s/step - loss: 0.1554 - accuracy: 0.9466 - precision: 0.9529 - recall: 0.9410 - val_loss: 1.0075 - val_accuracy: 0.7798 - val_precision: 0.7941 - val_recall: 0.7705\n",
      "Epoch 39/40\n",
      "670/670 [==============================] - 1350s 2s/step - loss: 0.1389 - accuracy: 0.9507 - precision: 0.9561 - recall: 0.9448 - val_loss: 1.1459 - val_accuracy: 0.7642 - val_precision: 0.7779 - val_recall: 0.7580\n",
      "Epoch 40/40\n",
      "670/670 [==============================] - 1352s 2s/step - loss: 0.1501 - accuracy: 0.9484 - precision: 0.9542 - recall: 0.9434 - val_loss: 0.9717 - val_accuracy: 0.7754 - val_precision: 0.7908 - val_recall: 0.7673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/virendrasinh10/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_generator,\n",
    "          epochs=40,\n",
    "          validation_data=validation_generator)\n",
    "try:\n",
    "    model.save(\"densenet.h5\")\n",
    "except:\n",
    "    model.save(\"/home/virendrasinh10/newmod.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " densenet169 (Functional)    (None, 7, 7, 1664)        12642880  \n",
      "                                                                 \n",
      " global_average_pooling2d_1  (None, 1664)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1024)              1704960   \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 1024)              4096      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 512)               2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               65664     \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14946250 (57.02 MB)\n",
      "Trainable params: 2340234 (8.93 MB)\n",
      "Non-trainable params: 12606016 (48.09 MB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/40\n",
      "670/670 [==============================] - 1221s 2s/step - loss: 2.6361 - accuracy: 0.4968 - precision_1: 0.6877 - recall_1: 0.3261 - val_loss: 2.3485 - val_accuracy: 0.4930 - val_precision_1: 0.6190 - val_recall_1: 0.3729 - lr: 0.0010\n",
      "Epoch 2/40\n",
      "670/670 [==============================] - 1214s 2s/step - loss: 1.9085 - accuracy: 0.5593 - precision_1: 0.7269 - recall_1: 0.3945 - val_loss: 1.8237 - val_accuracy: 0.5421 - val_precision_1: 0.6891 - val_recall_1: 0.4012 - lr: 0.0010\n",
      "Epoch 3/40\n",
      "670/670 [==============================] - 1214s 2s/step - loss: 1.6253 - accuracy: 0.5779 - precision_1: 0.7297 - recall_1: 0.4168 - val_loss: 1.7305 - val_accuracy: 0.5302 - val_precision_1: 0.6612 - val_recall_1: 0.3894 - lr: 0.0010\n",
      "Epoch 4/40\n",
      "670/670 [==============================] - 1246s 2s/step - loss: 1.5030 - accuracy: 0.5811 - precision_1: 0.7343 - recall_1: 0.4234 - val_loss: 1.5946 - val_accuracy: 0.5476 - val_precision_1: 0.6894 - val_recall_1: 0.4311 - lr: 0.0010\n",
      "Epoch 5/40\n",
      "670/670 [==============================] - 1213s 2s/step - loss: 1.4409 - accuracy: 0.5870 - precision_1: 0.7365 - recall_1: 0.4361 - val_loss: 1.5523 - val_accuracy: 0.5561 - val_precision_1: 0.6793 - val_recall_1: 0.4107 - lr: 0.0010\n",
      "Epoch 6/40\n",
      "670/670 [==============================] - 1220s 2s/step - loss: 1.3674 - accuracy: 0.6054 - precision_1: 0.7473 - recall_1: 0.4602 - val_loss: 1.3805 - val_accuracy: 0.5981 - val_precision_1: 0.7224 - val_recall_1: 0.4725 - lr: 0.0010\n",
      "Epoch 7/40\n",
      "670/670 [==============================] - 1211s 2s/step - loss: 1.3157 - accuracy: 0.6129 - precision_1: 0.7541 - recall_1: 0.4755 - val_loss: 1.4020 - val_accuracy: 0.5852 - val_precision_1: 0.7078 - val_recall_1: 0.4678 - lr: 0.0010\n",
      "Epoch 8/40\n",
      "670/670 [==============================] - 1209s 2s/step - loss: 1.2491 - accuracy: 0.6313 - precision_1: 0.7617 - recall_1: 0.4990 - val_loss: 1.4735 - val_accuracy: 0.5751 - val_precision_1: 0.6696 - val_recall_1: 0.4740 - lr: 0.0010\n",
      "Epoch 9/40\n",
      "670/670 [==============================] - 1214s 2s/step - loss: 1.2121 - accuracy: 0.6382 - precision_1: 0.7663 - recall_1: 0.5127 - val_loss: 1.2962 - val_accuracy: 0.6144 - val_precision_1: 0.7161 - val_recall_1: 0.5205 - lr: 0.0010\n",
      "Epoch 10/40\n",
      "670/670 [==============================] - 1218s 2s/step - loss: 1.1782 - accuracy: 0.6474 - precision_1: 0.7667 - recall_1: 0.5289 - val_loss: 1.3954 - val_accuracy: 0.5861 - val_precision_1: 0.7151 - val_recall_1: 0.4647 - lr: 0.0010\n",
      "Epoch 11/40\n",
      "670/670 [==============================] - 1228s 2s/step - loss: 1.1246 - accuracy: 0.6667 - precision_1: 0.7789 - recall_1: 0.5478 - val_loss: 1.4194 - val_accuracy: 0.5848 - val_precision_1: 0.6960 - val_recall_1: 0.4839 - lr: 9.0484e-04\n",
      "Epoch 12/40\n",
      "670/670 [==============================] - 1223s 2s/step - loss: 1.0889 - accuracy: 0.6786 - precision_1: 0.7801 - recall_1: 0.5651 - val_loss: 1.2521 - val_accuracy: 0.6311 - val_precision_1: 0.7468 - val_recall_1: 0.5269 - lr: 8.1873e-04\n",
      "Epoch 13/40\n",
      "670/670 [==============================] - 1204s 2s/step - loss: 1.0370 - accuracy: 0.6986 - precision_1: 0.7974 - recall_1: 0.5966 - val_loss: 1.1950 - val_accuracy: 0.6436 - val_precision_1: 0.7514 - val_recall_1: 0.5292 - lr: 7.4082e-04\n",
      "Epoch 14/40\n",
      "670/670 [==============================] - 1207s 2s/step - loss: 0.9978 - accuracy: 0.7050 - precision_1: 0.8016 - recall_1: 0.6088 - val_loss: 1.1835 - val_accuracy: 0.6419 - val_precision_1: 0.7406 - val_recall_1: 0.5546 - lr: 6.7032e-04\n",
      "Epoch 15/40\n",
      "670/670 [==============================] - 1221s 2s/step - loss: 0.9572 - accuracy: 0.7161 - precision_1: 0.8073 - recall_1: 0.6270 - val_loss: 1.1715 - val_accuracy: 0.6516 - val_precision_1: 0.7375 - val_recall_1: 0.5757 - lr: 6.0653e-04\n",
      "Epoch 16/40\n",
      "670/670 [==============================] - 1230s 2s/step - loss: 0.9246 - accuracy: 0.7265 - precision_1: 0.8122 - recall_1: 0.6442 - val_loss: 1.1281 - val_accuracy: 0.6512 - val_precision_1: 0.7408 - val_recall_1: 0.5778 - lr: 5.4881e-04\n",
      "Epoch 17/40\n",
      "670/670 [==============================] - 1208s 2s/step - loss: 0.8885 - accuracy: 0.7369 - precision_1: 0.8179 - recall_1: 0.6569 - val_loss: 1.1506 - val_accuracy: 0.6574 - val_precision_1: 0.7330 - val_recall_1: 0.5785 - lr: 4.9659e-04\n",
      "Epoch 18/40\n",
      "670/670 [==============================] - 1200s 2s/step - loss: 0.8504 - accuracy: 0.7492 - precision_1: 0.8256 - recall_1: 0.6778 - val_loss: 1.1356 - val_accuracy: 0.6756 - val_precision_1: 0.7435 - val_recall_1: 0.6098 - lr: 4.4933e-04\n",
      "Epoch 19/40\n",
      "670/670 [==============================] - 1202s 2s/step - loss: 0.8286 - accuracy: 0.7578 - precision_1: 0.8307 - recall_1: 0.6881 - val_loss: 1.0944 - val_accuracy: 0.6755 - val_precision_1: 0.7540 - val_recall_1: 0.6047 - lr: 4.0657e-04\n",
      "Epoch 20/40\n",
      "670/670 [==============================] - 1231s 2s/step - loss: 0.7875 - accuracy: 0.7740 - precision_1: 0.8391 - recall_1: 0.7049 - val_loss: 1.0341 - val_accuracy: 0.6865 - val_precision_1: 0.7608 - val_recall_1: 0.6197 - lr: 3.6788e-04\n",
      "Epoch 21/40\n",
      "670/670 [==============================] - 1222s 2s/step - loss: 0.7598 - accuracy: 0.7829 - precision_1: 0.8452 - recall_1: 0.7200 - val_loss: 1.1138 - val_accuracy: 0.6652 - val_precision_1: 0.7433 - val_recall_1: 0.5854 - lr: 3.3287e-04\n",
      "Epoch 22/40\n",
      "670/670 [==============================] - 1227s 2s/step - loss: 0.7364 - accuracy: 0.7919 - precision_1: 0.8506 - recall_1: 0.7316 - val_loss: 1.0522 - val_accuracy: 0.6954 - val_precision_1: 0.7507 - val_recall_1: 0.6426 - lr: 3.0119e-04\n",
      "Epoch 23/40\n",
      "670/670 [==============================] - 1225s 2s/step - loss: 0.7076 - accuracy: 0.7980 - precision_1: 0.8522 - recall_1: 0.7424 - val_loss: 1.0221 - val_accuracy: 0.6946 - val_precision_1: 0.7635 - val_recall_1: 0.6288 - lr: 2.7253e-04\n",
      "Epoch 24/40\n",
      "670/670 [==============================] - 1215s 2s/step - loss: 0.6882 - accuracy: 0.8062 - precision_1: 0.8601 - recall_1: 0.7517 - val_loss: 1.0075 - val_accuracy: 0.7073 - val_precision_1: 0.7648 - val_recall_1: 0.6550 - lr: 2.4660e-04\n",
      "Epoch 25/40\n",
      "670/670 [==============================] - 1227s 2s/step - loss: 0.6621 - accuracy: 0.8134 - precision_1: 0.8628 - recall_1: 0.7639 - val_loss: 0.9999 - val_accuracy: 0.7136 - val_precision_1: 0.7705 - val_recall_1: 0.6637 - lr: 2.2313e-04\n",
      "Epoch 26/40\n",
      "670/670 [==============================] - 1220s 2s/step - loss: 0.6375 - accuracy: 0.8223 - precision_1: 0.8705 - recall_1: 0.7764 - val_loss: 0.9806 - val_accuracy: 0.7195 - val_precision_1: 0.7736 - val_recall_1: 0.6690 - lr: 2.0190e-04\n",
      "Epoch 27/40\n",
      "670/670 [==============================] - 1218s 2s/step - loss: 0.6153 - accuracy: 0.8296 - precision_1: 0.8763 - recall_1: 0.7854 - val_loss: 1.0025 - val_accuracy: 0.7183 - val_precision_1: 0.7676 - val_recall_1: 0.6705 - lr: 1.8268e-04\n",
      "Epoch 28/40\n",
      "670/670 [==============================] - 1216s 2s/step - loss: 0.5932 - accuracy: 0.8358 - precision_1: 0.8795 - recall_1: 0.7961 - val_loss: 0.9932 - val_accuracy: 0.7229 - val_precision_1: 0.7682 - val_recall_1: 0.6847 - lr: 1.6530e-04\n",
      "Epoch 29/40\n",
      "670/670 [==============================] - 1218s 2s/step - loss: 0.5733 - accuracy: 0.8471 - precision_1: 0.8863 - recall_1: 0.8077 - val_loss: 0.9691 - val_accuracy: 0.7308 - val_precision_1: 0.7779 - val_recall_1: 0.6929 - lr: 1.4957e-04\n",
      "Epoch 30/40\n",
      "670/670 [==============================] - 1272s 2s/step - loss: 0.5560 - accuracy: 0.8519 - precision_1: 0.8891 - recall_1: 0.8149 - val_loss: 0.9988 - val_accuracy: 0.7136 - val_precision_1: 0.7634 - val_recall_1: 0.6690 - lr: 1.3534e-04\n",
      "Epoch 31/40\n",
      "670/670 [==============================] - 1255s 2s/step - loss: 0.5290 - accuracy: 0.8594 - precision_1: 0.8929 - recall_1: 0.8265 - val_loss: 0.9759 - val_accuracy: 0.7327 - val_precision_1: 0.7756 - val_recall_1: 0.6904 - lr: 1.2246e-04\n",
      "Epoch 32/40\n",
      "670/670 [==============================] - 1235s 2s/step - loss: 0.5128 - accuracy: 0.8647 - precision_1: 0.8968 - recall_1: 0.8322 - val_loss: 0.9707 - val_accuracy: 0.7388 - val_precision_1: 0.7762 - val_recall_1: 0.7052 - lr: 1.1080e-04\n",
      "Epoch 33/40\n",
      "670/670 [==============================] - 1234s 2s/step - loss: 0.5098 - accuracy: 0.8662 - precision_1: 0.8974 - recall_1: 0.8362 - val_loss: 0.9420 - val_accuracy: 0.7441 - val_precision_1: 0.7850 - val_recall_1: 0.7050 - lr: 1.0026e-04\n",
      "Epoch 34/40\n",
      "670/670 [==============================] - 1178s 2s/step - loss: 0.4937 - accuracy: 0.8743 - precision_1: 0.9024 - recall_1: 0.8432 - val_loss: 0.9503 - val_accuracy: 0.7415 - val_precision_1: 0.7805 - val_recall_1: 0.7043 - lr: 9.0718e-05\n",
      "Epoch 35/40\n",
      "670/670 [==============================] - 1199s 2s/step - loss: 0.4774 - accuracy: 0.8782 - precision_1: 0.9056 - recall_1: 0.8520 - val_loss: 0.9457 - val_accuracy: 0.7449 - val_precision_1: 0.7835 - val_recall_1: 0.7138 - lr: 8.2085e-05\n",
      "Epoch 36/40\n",
      "670/670 [==============================] - 1239s 2s/step - loss: 0.4590 - accuracy: 0.8870 - precision_1: 0.9147 - recall_1: 0.8611 - val_loss: 0.9349 - val_accuracy: 0.7443 - val_precision_1: 0.7839 - val_recall_1: 0.7121 - lr: 7.4273e-05\n",
      "Epoch 37/40\n",
      "670/670 [==============================] - 1231s 2s/step - loss: 0.4516 - accuracy: 0.8889 - precision_1: 0.9146 - recall_1: 0.8626 - val_loss: 0.9542 - val_accuracy: 0.7453 - val_precision_1: 0.7826 - val_recall_1: 0.7149 - lr: 6.7205e-05\n",
      "Epoch 38/40\n",
      "670/670 [==============================] - 1210s 2s/step - loss: 0.4449 - accuracy: 0.8884 - precision_1: 0.9151 - recall_1: 0.8638 - val_loss: 0.9574 - val_accuracy: 0.7511 - val_precision_1: 0.7804 - val_recall_1: 0.7185 - lr: 6.0810e-05\n",
      "Epoch 39/40\n",
      "670/670 [==============================] - 1274s 2s/step - loss: 0.4340 - accuracy: 0.8948 - precision_1: 0.9183 - recall_1: 0.8722 - val_loss: 0.9525 - val_accuracy: 0.7472 - val_precision_1: 0.7863 - val_recall_1: 0.7174 - lr: 5.5023e-05\n",
      "Epoch 40/40\n",
      "670/670 [==============================] - 1234s 2s/step - loss: 0.4187 - accuracy: 0.9013 - precision_1: 0.9222 - recall_1: 0.8785 - val_loss: 0.9491 - val_accuracy: 0.7536 - val_precision_1: 0.7827 - val_recall_1: 0.7269 - lr: 4.9787e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/virendrasinh10/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "\n",
    "from tensorflow.keras.applications import DenseNet169\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "input_shape = (224, 224, 3)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "base_model = DenseNet169(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "for layer in base_model.layers[:-5]:\n",
    "    layer.trainable = False\n",
    "model.add(base_model)\n",
    "\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(1024, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(512, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "def lr_schedule(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "\n",
    "adam_optimizer = Adam()\n",
    "\n",
    "\n",
    "model.compile(optimizer=adam_optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', Precision(), Recall()])\n",
    "\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "\n",
    "model.fit(train_generator,\n",
    "          epochs=40,\n",
    "          validation_data=validation_generator,\n",
    "          callbacks=[lr_scheduler])\n",
    "\n",
    "try:\n",
    "    model.save(\"densenet.h5\")\n",
    "except:\n",
    "    model.save(\"/home/virendrasinh10/newmod.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " densenet169 (Functional)    (None, 7, 7, 1664)        12642880  \n",
      "                                                                 \n",
      " global_average_pooling2d_2  (None, 1664)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1024)              1704960   \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 1024)              4096      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 512)               2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 128)               65664     \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14946250 (57.02 MB)\n",
      "Trainable params: 2340490 (8.93 MB)\n",
      "Non-trainable params: 12605760 (48.09 MB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-30 23:27:52.853822: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 19267584 exceeds 10% of free system memory.\n",
      "2024-03-30 23:28:03.773583: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 19267584 exceeds 10% of free system memory.\n",
      "2024-03-30 23:28:03.887630: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2024-03-30 23:28:03.916441: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-03-30 23:28:04.161050: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-03-30 23:28:04.561137: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 19267584 exceeds 10% of free system memory.\n",
      "2024-03-30 23:28:05.876065: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 19267584 exceeds 10% of free system memory.\n",
      "2024-03-30 23:28:15.122455: I external/local_xla/xla/service/service.cc:168] XLA service 0x73c504134e20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-03-30 23:28:15.122502: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1650, Compute Capability 7.5\n",
      "2024-03-30 23:28:15.135498: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1711821495.284229   20482 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4/670 [..............................] - ETA: 5:35 - loss: 4.9562 - accuracy: 0.2109 - precision_2: 0.4000 - recall_2: 0.1406         "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-30 23:28:18.571387: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 19267584 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 38/670 [>.............................] - ETA: 16:12 - loss: 3.8634 - accuracy: 0.3289 - precision_2: 0.5337 - recall_2: 0.2146"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/Media/ISIC_2019_Training_Input/new_data/tr.ipynb Cell 14\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/Media/ISIC_2019_Training_Input/new_data/tr.ipynb#X14sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m lr_scheduler \u001b[39m=\u001b[39m LearningRateScheduler(lr_schedule)\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/Media/ISIC_2019_Training_Input/new_data/tr.ipynb#X14sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39m# Train the model with the learning rate scheduler callback\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/mnt/Media/ISIC_2019_Training_Input/new_data/tr.ipynb#X14sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m model\u001b[39m.\u001b[39mfit(train_generator,\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/Media/ISIC_2019_Training_Input/new_data/tr.ipynb#X14sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m           epochs\u001b[39m=\u001b[39m\u001b[39m40\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/Media/ISIC_2019_Training_Input/new_data/tr.ipynb#X14sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m           validation_data\u001b[39m=\u001b[39mvalidation_generator,\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/Media/ISIC_2019_Training_Input/new_data/tr.ipynb#X14sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m           callbacks\u001b[39m=\u001b[39m[lr_scheduler])\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/Media/ISIC_2019_Training_Input/new_data/tr.ipynb#X14sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/Media/ISIC_2019_Training_Input/new_data/tr.ipynb#X14sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     model\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39mdensenet_approach2.h5\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1799\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1800\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1801\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1804\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1805\u001b[0m ):\n\u001b[1;32m   1806\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1807\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m   1808\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1809\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    866\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    867\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m   \u001b[39mreturn\u001b[39;00m tracing_compilation\u001b[39m.\u001b[39mcall_function(\n\u001b[1;32m    869\u001b[0m       args, kwds, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_config\n\u001b[1;32m    870\u001b[0m   )\n\u001b[1;32m    871\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    872\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mbind(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[39mreturn\u001b[39;00m function\u001b[39m.\u001b[39m_call_flat(  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[39m=\u001b[39mfunction\u001b[39m.\u001b[39mcaptured_inputs\n\u001b[1;32m    141\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1320\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1321\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1322\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inference_function\u001b[39m.\u001b[39mcall_preflattened(args)\n\u001b[1;32m   1324\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m     args,\n\u001b[1;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1327\u001b[0m     executing_eagerly)\n\u001b[1;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall_preflattened\u001b[39m(\u001b[39mself\u001b[39m, args: Sequence[core\u001b[39m.\u001b[39mTensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_flat(\u001b[39m*\u001b[39margs)\n\u001b[1;32m    217\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mcall_function(\n\u001b[1;32m    252\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname,\n\u001b[1;32m    253\u001b[0m         \u001b[39mlist\u001b[39m(args),\n\u001b[1;32m    254\u001b[0m         \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mflat_outputs),\n\u001b[1;32m    255\u001b[0m     )\n\u001b[1;32m    256\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[39mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mfunction_call_options\u001b[39m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[1;32m   1485\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute(\n\u001b[1;32m   1487\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1488\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   1489\u001b[0m       inputs\u001b[39m=\u001b[39mtensor_inputs,\n\u001b[1;32m   1490\u001b[0m       attrs\u001b[39m=\u001b[39mattrs,\n\u001b[1;32m   1491\u001b[0m       ctx\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m,\n\u001b[1;32m   1492\u001b[0m   )\n\u001b[1;32m   1493\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1495\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1496\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[1;32m   1501\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import DenseNet169\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "input_shape = (224, 224, 3)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "base_model = DenseNet169(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "for layer in base_model.layers[:-6]:\n",
    "    layer.trainable = False\n",
    "model.add(base_model)\n",
    "\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(1024, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(512, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "def lr_schedule(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "adam_optimizer = Adam()\n",
    "\n",
    "model.compile(optimizer=adam_optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', Precision(), Recall()])\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "model.fit(train_generator,\n",
    "          epochs=40,\n",
    "          validation_data=validation_generator,\n",
    "          callbacks=[lr_scheduler])\n",
    "\n",
    "try:\n",
    "    model.save(\"densenet_approach2.h5\")\n",
    "except:\n",
    "    model.save(\"/home/virendrasinh10/densenet_approach2.h5\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
